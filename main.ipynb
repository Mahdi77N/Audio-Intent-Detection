{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import keras\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten,\n",
    "                          GlobalAveragePooling2D, MaxPooling2D)\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(csv_path=\"./dsl_data/development.csv\", data=\"train\"):\n",
    "    \"\"\"Read the data, based on the \"development.csv\" file. it will read the audio files, and return two pandas dataFrame, x and y.\n",
    "       it will not normalize the data. also will not change the data to categorical.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str, optional): path to the \"development.csv\" file. Defaults to \"./dsl_data/development.csv\".\n",
    "\n",
    "    Returns:\n",
    "        pandas dataFrame: the x has all the features, excluding \"path\", \"action\", \"object\" and \"Id\".\n",
    "        the y has \"action\", \"object\", and \"intention\" which is a new feature by adding the other two columns.\n",
    "        int: sample rate\n",
    "    \"\"\"\n",
    "\n",
    "    development_pd = pd.read_csv(csv_path)\n",
    "    development_pd[\"Signal\"] = \"\"\n",
    "\n",
    "    for index, row in development_pd.iterrows():\n",
    "        wave, srr = librosa.load(row[\"path\"], mono=True, sr=None)\n",
    "        development_pd.at[index, \"Signal\"] = wave\n",
    "\n",
    "    if data == \"train\":\n",
    "        x = development_pd.drop([\"Id\", \"action\", \"object\", \"path\"], axis=1)\n",
    "        y = development_pd[[\"action\", \"object\"]]\n",
    "        y[\"intention\"] = development_pd[\"action\"] + development_pd[\"object\"]\n",
    "        return x, y, srr\n",
    "\n",
    "    else:\n",
    "        x = development_pd.drop([\"Id\", \"path\"], axis=1)\n",
    "        return x, srr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_audios(x, top_db=30, hop_length=50):\n",
    "    \"\"\"Trim the audios in the X.\n",
    "\n",
    "    Args:\n",
    "        x (pandas dataFrame): DataFrame consist of features, incuding the audios\n",
    "        top_db (int, optional): audio's db less than this number will consider as silent. Defaults to 30.\n",
    "        hop_length (int, optional): sensitivity of triming. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        pandas dataFrame: features DataFrame, with trimed audio in the \"Signal\" column.\n",
    "    \"\"\"\n",
    "    for index, row in x.iterrows():\n",
    "        wave, i = librosa.effects.trim(\n",
    "            row[\"Signal\"], top_db=top_db, hop_length=hop_length\n",
    "        )\n",
    "        x.at[index, \"Signal\"] = wave\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mfcc(x, srr, max_audio_len=3):\n",
    "    \"\"\"convert the audio signal to MFCC\n",
    "\n",
    "    Args:\n",
    "        x (pandas dataFrame): our data set\n",
    "        srr (int): sample rate\n",
    "        max_pad_len (int, optional): maximum width of MFCC. all MFCCs will be in width of max_pad_len. Defaults to 215.\n",
    "\n",
    "    Returns:\n",
    "        pandas dataFrame: our data set\n",
    "    \"\"\"\n",
    "    for index, row in x.iterrows():\n",
    "        mfcc = librosa.feature.mfcc(row[\"Signal\"], sr=srr)\n",
    "        max_pad_len = (max_audio_len * srr) / 512\n",
    "        pad_width = ceil(max_pad_len - mfcc.shape[1])\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "        x.at[index, \"Signal\"] = np.array(mfcc)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(x, y, x_columns, y_columns):\n",
    "    \"\"\"Convert our data from pandas dataFrame to numpy array. Also, will drop all columns execpt the x_columns and y_columns\n",
    "\n",
    "    Args:\n",
    "        x (pandas dataFrame): our train set's features\n",
    "        y (pandas dataFrame): train set's classes\n",
    "        x_columns (array): features in train set that we want to train the model with them.\n",
    "        y_columns (array): one of three classes, from \"object\", \"action\", and \"intention\".\n",
    "\n",
    "    Returns:\n",
    "        numpay arrays: will return x and y in numpy arrays and only with specified columns\n",
    "    \"\"\"\n",
    "    x_temp = x[x_columns]\n",
    "    # x_temp = x_temp.to_numpy()\n",
    "    # print(x_temp)\n",
    "    print(len(x_temp))\n",
    "    x_temp_2 = np.array([x_temp.loc[i][\"Signal\"] for i in range(len(x_temp))])\n",
    "    # print(x_temp.shape)\n",
    "\n",
    "    y_temp = y[y_columns]\n",
    "    y_temp = y_temp.to_numpy()\n",
    "\n",
    "    return x_temp_2, y_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_oneHot(y):\n",
    "    \"\"\"encode the classes from strings to oneHot\n",
    "\n",
    "    Args:\n",
    "        y (numpy array): train set's classes\n",
    "\n",
    "    Returns:\n",
    "        numpy array: train set's classes in oneHot format.\n",
    "        Label encoder Object: in order to be able to inverse oneHot to strings when we have the results.\n",
    "\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    temp = le.fit_transform(y)\n",
    "    y_oneHot = to_categorical(temp, num_classes=max(temp) + 1)\n",
    "\n",
    "    return y_oneHot, le\n",
    "\n",
    "\n",
    "# tt = le.inverse_transform(t)\n",
    "# print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            32,\n",
    "            kernel_size=(2, 2),\n",
    "            activation=\"relu\",\n",
    "            input_shape=input_shape,\n",
    "            strides=(1, 2),\n",
    "            padding=\"same\",\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            48, kernel_size=(2, 2), activation=\"relu\", strides=(1, 2), padding=\"same\"\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            120, kernel_size=(2, 2), activation=\"relu\", strides=(1, 2), padding=\"same\"\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(x, y, max_length=3):\n",
    "    t = []\n",
    "    for index, row in X_trimed.iterrows():\n",
    "        t.append(librosa.get_duration(row[\"Signal\"], sr=srr))\n",
    "\n",
    "    ouliers_index = np.where(np.array(t) > 3)\n",
    "\n",
    "    x_removed_ouliers = x.copy().drop(ouliers_index[0], axis=0)\n",
    "    y_removed_ouliers = y.copy().drop(ouliers_index[0], axis=0)\n",
    "\n",
    "    x_removed_ouliers = x_removed_ouliers.reset_index()\n",
    "    y_removed_ouliers = y_removed_ouliers.reset_index()\n",
    "    return x_removed_ouliers, y_removed_ouliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehtes\\AppData\\Local\\Temp\\ipykernel_1436\\2289199453.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"intention\"] = development_pd[\"action\"] + development_pd[\"object\"]\n"
     ]
    }
   ],
   "source": [
    "X, Y, srr = read_data()\n",
    "X_trimed = trim_audios(X.copy(), top_db=10, hop_length=10)\n",
    "X_no_ouliers, y_no_ouliers = remove_outliers(X_trimed, Y)\n",
    "X_mfcc = convert_to_mfcc(X_no_ouliers, srr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mfcc = X_mfcc.reset_index()\n",
    "y_no_ouliers = y_no_ouliers.reset_index()\n",
    "X_n, Y_n = convert_to_numpy(X_mfcc, Y, [\"Signal\"], [\"intention\"])\n",
    "y_oneHpt, le = convert_y_to_oneHot(Y_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_1 = X_n.shape[1]\n",
    "dim_2 = X_n.shape[2]\n",
    "channels = 1\n",
    "classes = y_oneHpt.shape[1]\n",
    "X_n = X_n.reshape((X_n.shape[0], dim_1, dim_2, channels))\n",
    "input_shape = (dim_1, dim_2, channels)\n",
    "\n",
    "cnn_model = get_cnn_model(input_shape, classes)\n",
    "print(cnn_model.summary())\n",
    "\n",
    "keras_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"./Graph\", histogram_freq=1, write_graph=True, write_images=True\n",
    ")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"./audio_classification.hdf5\", verbose=1, save_best_only=True\n",
    ")\n",
    "h = cnn_model.fit(\n",
    "    X_n,\n",
    "    y_oneHpt,\n",
    "    batch_size=8,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[keras_callback, checkpointer],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c663c866a1fb96859a8987df8dcaf456bb4d601a775f22c0ae8b5e612b0ffc21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
